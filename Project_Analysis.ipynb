{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽµ Cortex Music Generator: Full Pipeline Analysis\n",
                "\n",
                "This notebook demonstrates the **end-to-end flow** of the project, connecting the pre-trained Emotion CNN with the Data-Driven TRPO Music Generator.\n",
                "\n",
                "### ðŸš€ The Pipeline\n",
                "1.  **Input**: An image (User's face) + Instrument selection.\n",
                "2.  **The Eye (CNN)**: Analyzes the image using `best_model.keras` to detect the **Emotion**.\n",
                "3.  **The Knowledge (Data)**: Looks up the **Tempo & Timbre** stats for the selected instrument from `final2.O...csv`.\n",
                "4.  **The Brain (TRPO)**: The Reinforcement Learning agent uses the *Emotion* + *Stats* to generate a unique melody.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import cv2\n",
                "\n",
                "# Import our custom backend library\n",
                "from project_backend import DataHandler, EmotionCNN, TRPOAgent, SWARAS, EMOTIONS\n",
                "\n",
                "# File Paths\n",
                "CSV_PATH = 'final2.O_merged_instrument_dataset(2054 audios).csv'\n",
                "MODEL_PATH = 'best_model.keras'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ðŸ‘ï¸ The Eye: Emotion Detection (`best_model.keras`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the CNN\n",
                "cnn = EmotionCNN(MODEL_PATH)\n",
                "\n",
                "if cnn.is_trained:\n",
                "    print(f\"âœ… Loaded CNN Model from {MODEL_PATH}\")\n",
                "    \n",
                "    # --- SIMULATION ---\n",
                "    # In the real app, this comes from the Camera.\n",
                "    # Here, we generate a random 'image' to prove the model accepts input.\n",
                "    dummy_face_image = np.random.randint(0, 255, (200, 200, 3), dtype=np.uint8)\n",
                "    \n",
                "    # Predict\n",
                "    detected_emotion = cnn.predict(dummy_face_image)\n",
                "    print(f\"\\nðŸ–¼ï¸ Simulated Input Processed.\")\n",
                "    print(f\"ðŸ§  Model Prediction: **{detected_emotion.upper()}**\")\n",
                "else:\n",
                "    print(\"âš ï¸ Model not found. Using random fallback.\")\n",
                "    detected_emotion = \"happy\" # fallback for demo"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ðŸ“Š The Knowledge: Instrument Data\n",
                "The UI asks the user for an **Instrument** (e.g., Sitar). We use the CSV to find out how a Sitar should sound (Tempo)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# User Selection (Simulating UI Dropdown)\n",
                "selected_instrument = \"Sitar\" \n",
                "\n",
                "# Load Stats\n",
                "handler = DataHandler(CSV_PATH)\n",
                "handler.load_data()\n",
                "\n",
                "instrument_features = handler.get_instrument_features(selected_instrument)\n",
                "print(f\"ðŸŽ» Selected Instrument: {selected_instrument}\")\n",
                "print(f\"ðŸ“Š Data Stats -> Target Tempo: {instrument_features.get('tempo', 90):.1f} BPM\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ðŸ§  The Brain: TRPO Agent Training\n",
                "Now we pass both inputs to the Agent:\n",
                "*   **Input 1**: Emotion (`{detected_emotion}`) -> Defines the Raga (Scale).\n",
                "*   **Input 2**: Instrument Stats -> Defines the Tempo and Note transitions.\n",
                "\n",
                "The Agent now \"practices\" (trains) to optimize for these targets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Agent\n",
                "agent = TRPOAgent(state_size=7, action_size=7)\n",
                "\n",
                "print(f\"ðŸ¤– Training TRPO Agent for context: [{detected_emotion.upper()}] + [{selected_instrument.upper()}]...\")\n",
                "\n",
                "# Train on the data features\n",
                "agent.train_from_data(instrument_features, detected_emotion, episodes=200)\n",
                "print(\"âœ… Training Complete.\")\n",
                "\n",
                "# Visualize the learned Policy (The \"Brain\")\n",
                "# This heatmap shows which note (X-axis) the agent is likely to play after the current note (Y-axis)\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(agent.policy, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
                "            xticklabels=list(SWARAS.keys()), yticklabels=list(SWARAS.keys()))\n",
                "plt.title(f\"Learned Policy for {detected_emotion.title()} {selected_instrument}\")\n",
                "plt.xlabel(\"Next Note (Action)\")\n",
                "plt.ylabel(\"Current Note (State)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ðŸŽ¶ The Voice: Generation\n",
                "Finally, we use the trained agent plus the synthesis logic to produce audio."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from project_backend import generate_session\n",
                "\n",
                "# This function wraps the Training + Generation process used in the App\n",
                "audio, rate, policy = generate_session(detected_emotion, selected_instrument, handler, duration=5)\n",
                "\n",
                "print(f\"ðŸŽµ Generated {len(audio)/rate:.1f} seconds of audio.\")\n",
                "\n",
                "# Plot Waveform\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.plot(audio[:rate*2]) # First 2 seconds\n",
                "plt.title(f\"Generated Audio Waveform (First 2s) - {detected_emotion} {selected_instrument}\")\n",
                "plt.xlabel(\"Samples\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}